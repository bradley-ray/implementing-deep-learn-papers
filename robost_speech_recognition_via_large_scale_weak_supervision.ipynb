{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe5ARfiWy1LCvvLJcn+3eN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with fine-tuning OpenAI Whisper\n",
        "\n",
        "- [whisper paper](https://arxiv.org/abs/2212.04356)\n",
        "- [openai github](https://github.com/openai/whisper)"
      ],
      "metadata": {
        "id": "v4c98eRp1wFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crejdxnr1jbu"
      },
      "outputs": [],
      "source": [
        "# tiny multilingual model\n",
        "# !wget https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
        "# tiny english only model\n",
        "!wget https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math"
      ],
      "metadata": {
        "id": "tIKh51TD12Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.load('tiny.en.pt')\n",
        "for layer in data['model_state_dict']:\n",
        "    print(data['model_state_dict'][layer].shape, layer)"
      ],
      "metadata": {
        "id": "kwiFCy27145E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttn(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert(emb_dim % num_heads == 0)\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.query = nn.Linear(emb_dim, emb_dim)\n",
        "        # why no bias for key?\n",
        "        self.key = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.value = nn.Linear(emb_dim, emb_dim)\n",
        "        self.out = nn.Linear(emb_dim, emb_dim)\n",
        "        self.scores = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (bs, seq_len, emb_dim) -> (bs, seq_len, num_heads, head_size)\n",
        "        q, k, v = [y(x).reshape(x.shape[0], -1, \n",
        "                                self.num_heads, self.emb_dim//self.num_heads)\n",
        "                    for y in [self.query, self.key, self.value]]\n",
        "        # (bs, num_heads, seq_len, head_size)\n",
        "        q, k, v = [y.permute(0, 2, 1, 3) for y in [q, k, v]]\n",
        "        out = q @ k.transpose(-2, -1) * (1/math.sqrt(q.shape[-1]))\n",
        "        out = torch.softmax(out, dim=-1)\n",
        "        self.scores = out.clone()\n",
        "        # back to (bs, seq_len, emb_dim)\n",
        "        out = (out @ v).permute(0, 2, 1, 3)\n",
        "        out = out.reshape(x.shape[0], -1, self.emb_dim)\n",
        "        out = self.out(out)\n",
        "        return out\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.ff1 = nn.Linear(emb_dim, emb_dim)\n",
        "        self.ff2 = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.droupout(F.gelu(self.ff1), probability=0.1, train=False)\n",
        "        x = F.droupout(self.ff2, probability=0.1, train=False)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_ch, emb_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttn(emb_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = MLPBlock(emb_dim)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, emb_dim, num_heads, num_encs):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_encs = num_encs\n",
        "        self.emb = nn.Sequential(*[\n",
        "            nn.Conv1d(in_ch, emb_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(emb_dim, emb_dim, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "        ])\n",
        "        # not implementing sin_pos_emb b/c \n",
        "        # pretrained weights already provide pos_emb values\n",
        "        self.pos_emb = self.register_buffer('pos_emb', \n",
        "                                            torch.zeros((1500,self.emb_dim)))\n",
        "        self.model = nn.Sequential(*[\n",
        "            EncoderBlock(in_ch, emb_dim, num_heads)\n",
        "            for _ in range(num_encs)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x) + self.pos_emb\n",
        "        x = self.model(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def load_pretrained_model(self, path):\n",
        "        ..."
      ],
      "metadata": {
        "id": "ty4fCK7B18ya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}