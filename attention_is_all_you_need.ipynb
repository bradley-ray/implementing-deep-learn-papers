{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VbM8_ZmHLJlv",
        "HTAsSn4ULe8d"
      ],
      "authorship_tag": "ABX9TyM69+MzCVjx24fmpRShpedz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchdata\n",
        "# !pip install -U torchtext\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCck_6Rqxm6l",
        "outputId": "50bcde38-b37f-4b9b-ea10-a9fe4fafed7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 23.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b912e9a18e2098aab8356a59debe162857916527a86688d233dfb6fb536607ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hb1o6IFg5URr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os\n",
        "\n",
        "import torchtext.transforms as T\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from torchtext.vocab import vocab\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Setup"
      ],
      "metadata": {
        "id": "VbM8_ZmHLJlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!rm -rf *.tgz*\n",
        "!wget https://www.statmt.org/europarl/v7/fr-en.tgz\n",
        "!tar xfz *.tgz\n",
        "!mkdir -p data/training\n",
        "!mv *.en *.fr data/training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-3jCgcw77wD",
        "outputId": "ae048205-99e5-4d05-ce18-d172d1fe2986"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 02:47:04--  https://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘fr-en.tgz’\n",
            "\n",
            "fr-en.tgz           100%[===================>] 193.33M  1.16MB/s    in 3m 55s  \n",
            "\n",
            "2022-11-07 02:50:59 (844 KB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: need to clean and speed this up, but for now it works fine\n",
        "def get_phrases(path, tokenizer, size):\n",
        "    phrases = []\n",
        "    en_tokenize = get_tokenizer(tokenizer, language='en')\n",
        "    fr_tokenize = get_tokenizer(tokenizer, language='fr')\n",
        "    # get tokenized dataset\n",
        "    with open(f'{path}/europarl-v7.fr-en.en') as en:\n",
        "        with open(f'{path}/europarl-v7.fr-en.fr') as fr:\n",
        "            k = 0\n",
        "            for phrase in zip(en.readlines(), fr.readlines()):\n",
        "                en_phrase = en_tokenize(phrase[0].strip())\n",
        "                fr_phrase = fr_tokenize(phrase[1].strip())\n",
        "                if len(en_phrase) > 16 or len(fr_phrase) > 16:\n",
        "                    continue\n",
        "                phrases.append({\n",
        "                    'en': en_phrase,\n",
        "                    'fr': fr_phrase,\n",
        "                })\n",
        "                k+=1\n",
        "                if k >= size: break\n",
        "    return phrases\n",
        "\n",
        "def get_vocab(phrases, special_tokens):\n",
        "    vocab_freq_en = {}\n",
        "    vocab_freq_fr = {}\n",
        "\n",
        "    for phrase in phrases:\n",
        "        for word in phrase['en']:\n",
        "            vocab_freq_en[word] = vocab_freq_en.get(word, 0) + 1\n",
        "\n",
        "        for word in phrase['fr']:\n",
        "            vocab_freq_fr[word] = vocab_freq_fr.get(word, 0) + 1\n",
        "\n",
        "    vocab_en_ = sorted(vocab_freq_en.keys(), key=lambda x: vocab_freq_en[x], reverse=True)\n",
        "    vocab_fr_ = sorted(vocab_freq_fr.keys(), key=lambda x: vocab_freq_fr[x], reverse=True)\n",
        "\n",
        "    \n",
        "\n",
        "    vocab_en = vocab_en_[:math.floor(0.6*len(vocab_en_))] + special_tokens\n",
        "    vocab_fr = vocab_fr_[:math.floor(0.6*len(vocab_fr_))] + special_tokens\n",
        "\n",
        "    vocab_en = vocab(OrderedDict([(word, 1) for word in vocab_en]))\n",
        "    idx = vocab_en[special_tokens[2]]\n",
        "    print(idx)\n",
        "    vocab_en.set_default_index(idx)\n",
        "    \n",
        "    vocab_fr = vocab(OrderedDict([(word, 1) for word in vocab_fr]))\n",
        "    idx = vocab_fr[special_tokens[2]]\n",
        "    print(idx)\n",
        "    vocab_fr.set_default_index(vocab_fr[special_tokens[-1]])\n",
        "\n",
        "    return vocab_en, vocab_fr\n",
        "\n",
        "\n",
        "def to_idx(phrases, vocab, special_tokens):\n",
        "    max_len = 0\n",
        "    idx_phrases = []\n",
        "    vocab_en, vocab_fr = vocab\n",
        "    for phrase in phrases:\n",
        "        phrase['en'] = special_tokens[:1] + phrase['en'] + special_tokens[1:2]\n",
        "        phrase['fr'] = special_tokens[:1] + phrase['fr'] + special_tokens[1:2]\n",
        "\n",
        "        if max_len < len(phrase['en']):\n",
        "            max_len = len(phrase['en'])\n",
        "        if max_len < len(phrase['fr']):\n",
        "            max_len = len(phrase['fr'])\n",
        "\n",
        "    for phrase in phrases:\n",
        "        en_phrase = phrase['en']\n",
        "        fr_phrase = phrase['fr']\n",
        "\n",
        "        en_phrase_idx = T.VocabTransform(vocab_en)(en_phrase)\n",
        "        fr_phrase_idx = T.VocabTransform(vocab_fr)(fr_phrase)\n",
        "\n",
        "        idx_phrases.append({\n",
        "            'en': en_phrase_idx,\n",
        "            'fr': fr_phrase_idx,\n",
        "        })\n",
        "        \n",
        "    return idx_phrases, max_len\n",
        "\n",
        "class WMT2014(Dataset):\n",
        "    def __init__(self, path, special_tokens, ds_len=10_000, show_idx=True):\n",
        "        self.path = path\n",
        "        self.show_idx = show_idx\n",
        "        self.special_tokens = special_tokens\n",
        "        self.phrases = get_phrases(self.path, 'moses', ds_len)\n",
        "        self.vocab = get_vocab(self.phrases, self.special_tokens)\n",
        "        self.idx_phrases, self.max_len = to_idx(self.phrases, self.vocab, self.special_tokens)\n",
        "\n",
        "    def __getitem__(self, idx) :\n",
        "        if self.show_idx:\n",
        "            padded_src = torch.zeros(self.max_len, dtype=torch.int32)\n",
        "            padded_tgt = torch.zeros(self.max_len, dtype=torch.int32)\n",
        "            src = torch.tensor(self.idx_phrases[idx]['en'])\n",
        "            tgt = torch.tensor(self.idx_phrases[idx]['fr'])\n",
        "            \n",
        "            padded_src[:src.shape[0]] = src\n",
        "            padded_tgt[:tgt.shape[0]] = tgt\n",
        "\n",
        "            return padded_src, padded_tgt\n",
        "        else:\n",
        "            src = self.phrases[idx]['en']\n",
        "            tgt = self.phrases[idx]['fr']\n",
        "            return src, tgt\n",
        "        \n",
        "\n",
        "        return src, tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.phrases)"
      ],
      "metadata": {
        "id": "N58foGL87-oR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model"
      ],
      "metadata": {
        "id": "3GTncQK4LOe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: add masking\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, num_heads, mask=False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def reshape(self, x):\n",
        "        # [bs, seq_len, num_heads, d_k]\n",
        "        # split into seperate heads\n",
        "        out = x.reshape(*x.shape[:2], self.num_heads, self.d_k)\n",
        "        # [bs, num_heads, seq_len, d_k]\n",
        "        # swap heads/seq_len dim to be able to do matmul on each head in parallel\n",
        "        return out.permute(0, 2, 1, 3)\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "        scale = 1 / self.d_k**0.5\n",
        "        out = q @ k.transpose(-2, -1) * scale\n",
        "        out = self.softmax(out) @ v\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # [bs, num_heads, seq_len, d_k]\n",
        "        q = self.reshape(self.w_q(q))\n",
        "        k = self.reshape(self.w_k(k))\n",
        "        v = self.reshape(self.w_v(v))\n",
        "\n",
        "        # [bs, seq_len, num_heads, d_k]\n",
        "        # compute attention and swap back to orig dims\n",
        "        attn = self.attention(q, v, k).permute(0, 2, 1, 3)\n",
        "        # [bs, seq_len, d_model]\n",
        "        # concatenate heads\n",
        "        attn = attn.reshape(-1, self.seq_len, self.d_model)\n",
        "\n",
        "        out = self.w_o(attn)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, emb_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.pe = torch.zeros(seq_len, emb_dim).cuda()\n",
        "        pos = torch.arange(seq_len).unsqueeze(1).cuda()\n",
        "        i = torch.arange(0, emb_dim, 2).cuda()\n",
        "        div = torch.exp(i * -1 * math.log(10_000) / emb_dim).cuda()\n",
        "        self.pe[:, 0::2] = torch.sin(pos / div).cuda()\n",
        "        self.pe[:, 1::2] = torch.cos(pos / div).cuda()\n",
        "\n",
        "        # give it batch dim\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, emb):\n",
        "        return emb + self.pe, self.pe\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, num_heads):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        norm_shape = (seq_len, d_model)\n",
        "\n",
        "        self.attn = MultiHeadAttention(seq_len, d_model, num_heads)\n",
        "        self.ln_1 = nn.LayerNorm(norm_shape)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, d_model),\n",
        "        )\n",
        "\n",
        "        self.ln_2 = nn.LayerNorm(norm_shape)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        attn = self.attn(src, src, src)\n",
        "        out_1 = self.ln_1(src + attn)\n",
        "\n",
        "        ff = self.ff(out_1)\n",
        "        out_2 = self.ln_2(out_1 + ff)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, num_heads):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \n",
        "        norm_shape = (seq_len, d_model)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(seq_len, d_model, num_heads)\n",
        "        self.ln_1 = nn.LayerNorm(norm_shape)\n",
        "\n",
        "        self.attn_2 = MultiHeadAttention(seq_len, d_model, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(norm_shape)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, d_model),\n",
        "        )\n",
        "        self.ln_3 = nn.LayerNorm(norm_shape)\n",
        "\n",
        "    # TODO: find better solutions for decoder inputs\n",
        "    def forward(self, args):\n",
        "        enc = args[0]\n",
        "        tgt = args[1]\n",
        "\n",
        "        attn_1 = self.attn_1(tgt, tgt, tgt)\n",
        "        out_1 = self.ln_1(enc + attn_1)\n",
        "\n",
        "        attn_2 = self.attn_2(enc, enc, out_1)\n",
        "        out_2 = self.ln_2(out_1 + attn_2)\n",
        "\n",
        "        ff = self.ff(out_2)\n",
        "        out_3 = self.ln_3(out_2 + ff)\n",
        "\n",
        "        return enc, out_3\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, seq_len, vocab_len, d_model, num_blocks, num_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.inp_emb = nn.Embedding(vocab_len[0], d_model)\n",
        "        self.out_emb = nn.Embedding(vocab_len[1], d_model)\n",
        "        self.pos_enc = PositionalEncoding(seq_len, d_model)\n",
        "\n",
        "        self.enc = nn.Sequential(\n",
        "            *[EncoderBlock(seq_len, d_model, num_heads) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.dec = nn.Sequential(\n",
        "            *[DecoderBlock(seq_len, d_model, num_heads) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(d_model, vocab_len[1]),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        inp_emb, _ = self.pos_enc(self.inp_emb(src))\n",
        "        enc = self.enc(inp_emb)\n",
        "\n",
        "        out_emb = self.out_emb(tgt)\n",
        "        _, dec = self.dec((enc, out_emb))\n",
        "        out = self.out(dec)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "t5iFypqFG8k5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "HTAsSn4ULe8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WMT2014('data/training', ['<SOS>', '<EOS>', '<UNK>'], ds_len=100_000)"
      ],
      "metadata": {
        "id": "0htcaD1C8AgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb30a96-966f-4f82-8228-a511b22c8867"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14349\n",
            "19148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dl = DataLoader(ds, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "oTLaQwa-8DFI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = ds[0][0].size(0)\n",
        "vocab_len = len(ds.vocab[0]), len(ds.vocab[1])\n",
        "d_model = 512\n",
        "num_blks = 6\n",
        "num_heads = 8\n",
        "\n",
        "transformer = Transformer(seq_len, vocab_len, d_model, num_blks, num_heads).to('cuda')\n",
        "opt = optim.Adam(transformer.parameters(), lr=2e-3, betas=(0.9,0.98))\n",
        "\n",
        "kl = nn.KLDivLoss()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    loss_ = 0\n",
        "    # currently just passing in tgt unmasked & unshifted\n",
        "    # model currently just guessing same statement for every input\n",
        "    # TODO: fix these issues\n",
        "    for batch_num, (src, tgt) in enumerate(dl):\n",
        "        transformer.train()\n",
        "        # TODO: find better fix for negative indicies\n",
        "        src[src<0] = ds.vocab[0]['<UNK>']\n",
        "        tgt[tgt<0] = ds.vocab[1]['<UNK>']\n",
        "\n",
        "        src = src.cuda()\n",
        "        tgt = tgt.cuda()\n",
        "        \n",
        "        opt.zero_grad()\n",
        "\n",
        "        out = transformer(src, tgt)\n",
        "\n",
        "        tgt_dist = torch.zeros_like(out).cuda()\n",
        "        for i in range(tgt.size(0)):\n",
        "            tgt_dist[i, :, :] = F.one_hot(tgt[i,:].long(), num_classes=tgt_dist.size(2))\n",
        "\n",
        "        loss = kl(out, tgt_dist)\n",
        "        loss_ = loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "    \n",
        "    if (epoch+1) % 2 == 0:\n",
        "        print(loss_)\n",
        "\n"
      ],
      "metadata": {
        "id": "9Sc9Do-4W3RJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0347f191-8659-47e1-eed8-1df8fb2e735b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
            " 20%|██        | 2/10 [07:13<28:55, 216.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002253243583254516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 4/10 [14:28<21:44, 217.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00019704042642842978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [21:44<14:30, 217.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002007755101658404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 8/10 [28:59<07:15, 217.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002167060156352818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [36:14<00:00, 217.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0002177873975597322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING MODEL RUNS\n",
        "\n",
        "transformer.eval()\n",
        "\n",
        "src = ds[10000][0].unsqueeze(0).cuda()\n",
        "tgt = ds[10000][1].unsqueeze(0).cuda()\n",
        "\n",
        "out = transformer(src, tgt)\n",
        "\n",
        "out.argmax(-1)\n",
        "\n",
        "def token_to_str(lang='en'):\n",
        "    def func(token):\n",
        "        if lang == 'en':\n",
        "            out = ds.vocab[0].lookup_token(token)\n",
        "        elif lang == 'fr':\n",
        "            out = ds.vocab[1].lookup_token(token)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    return func\n",
        "\n",
        "print('en:', ' '.join(map(token_to_str('en'), src[0])))\n",
        "print('fr:', ' '.join(map(token_to_str('fr'), out[0].argmax(-1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiqn_IDm0Wxy",
        "outputId": "022902bc-8067-4c2c-d2e6-dba0e623bb24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en: <SOS> In our opinion this initiative would be excessively bureaucratic and would not make sense . <EOS> .\n",
            "fr: <SOS> Il &apos;est , , . . . . . . . . . . . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "# tgt = torch.cat([ds[i][1].unsqueeze(0) for i in range(10)], dim=0)\n",
        "# out = torch.zeros(10, 18, 6500)\n",
        "\n",
        "# print('tgt.shape', tgt.shape)\n",
        "# print('out.shape', out.shape)\n",
        "\n",
        "# for i in range(tgt.size(0)):\n",
        "    # out[i, :, :] = F.one_hot(tgt[i, :].long(), num_classes=6500)"
      ],
      "metadata": {
        "id": "g83IHwkSszhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}